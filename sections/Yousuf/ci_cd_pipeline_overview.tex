
\section{Implementation of CI/CD pipeline via GitHub Actions for AWS [ Md Yousuf Ali Khan]}\label{sec:chapter_1}
For Continuous Integration and Continuous deployment, we used GitHub actions. Github action is event-driven means we can run series of tasks in a reaction to a specific event. An event triggers a workflow, which contains a job. A job consists of multiple steps which control the order of actions.

Workflows can contain multiple jobs which can be triggered or scheduled based on events. An event can be internal or external. A job is a set of steps. Workflow with multiple jobs by default will run those jobs on parallel, but we can change this behavior and make it sequential by making a job dependent on other jobs. This will also ensure that if a parent job fails, all the other jobs dependent on that job will not even run. The main goal of a Step is to run commands in a job. In comparison, actions are standalone commands that steps are made of. 

While designing our pipeline, we considered any push to our main branch as an event that will trigger our workflow. One of our starting jobs is to building our application and run the tests. If our tests failed or any kind of error occurred during the building of our application, we will generate a report consisting of the stacktrace and publish it as an artifact that can be downloaded anytime for debugging purposes. We also introduced caching for our application packages which will speed up the time to run our pipeline. Next, We will create a docker repository in AWS ECR for our application images. We had to provide our AWS account access key and secret access key for doing so from our pipeline. We used another feature of Github called secrets to provide those values, enabling us to use this sort of private value without exposing it. We created four of those secrets for our account id, access key id, secret access key, and AWS region. The commands that we had to run for our CDK to create and deploy our resources to AWS were becoming quite long. So we decided to create an npm repo inside the root of our CDK folder, which will help us hide those long commands behind short npm commands. Some of our CDK commands need variable parameters even that we can pass from our npm commands, making our pipeline much more readable. Also, we made this job depend on our pipeline's build job, which ensures that we will not run this job if our build job fails for any reason. The following job of our pipeline will create a network stack in our AWS account. We provide all the necessary secrets for building that stack and provide the environment as an argument in our npm command, enabling us to create different deployment environments for our application. The next step is to create a database stack. For deploying that stack, we passed two parameters, environment and application name to our CDK so that it can create a database for using that application name and using the environment variable that will be able to create that database to a specific environment. Our pipeline's last job is to build our application image and push that one to our AWS ECR repository that we created in one of the previous steps. We will build a new image for every release of our application and tag it with a unique name, which we will create using the current date and time along with the unique SHA value for that commit. Finally, we will clear the old parameter stacks that we used for publishing some of the resources ids and other properties that we wanted to share between multiple stacks.

In the next subsections, we will briefly  describe the implementation stages of our implementation of the automated software release in AWS cloud. 

