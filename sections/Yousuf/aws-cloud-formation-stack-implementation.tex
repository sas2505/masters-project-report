\subsection{AWS CloudFormation Stack Implementation [ Md Yousuf Ali Khan]}\label{sec:aws-cloudformation-stack}
As we choose AWS as our cloud provider, there were multiple options for implementing our IAAC. For example, we could use Terraform or AWS CloudFormation templates using JSON and YAML, but we found a better solution called AWS CDK(AWS Cloud Development Kit). AWS CDK is a software development framework for defining cloud infrastructure resources in code and deploying all those resources using AWS CloudFromation. In AWS CDK, we can write our infrastructure code in different programming languages like TypeScript, JavaScript, Python, Java, and C#. Even though TypeScript is the first language supported for developing AWS CDK applications, we choose Java as our AWS CDK language as our application was written in Java. We created a separate project for our infrastructure code in a folder called 'CDK.' Few significant benefits of using CDK are that we can use our object-oriented programming techniques to model your system using our desired programming language.
Furthermore, we can create and define high-level abstractions of our infrastructure, which will enable us to reuse those constructs in the future. We can even share or publish those constructs for public use.  A CDK construct is a basic building block of AWS CDK apps which is a single pre-configured CloudFormation resource or a stack that can be combined in any way to create any desired infrastructure. There are three different levels of constructs: Level 1 constructs, Level 2 constructs, and Level 3 constructs. Level 1 constructs are the direct equivalent of an underlying CloudFormation resource.
In contrast, Level 2 constructs consist of multiple Level 1 constructs; as a result, we can use those Level 2 constructs instead of creating all the Level 1 constructs manually by ourselves. Level 3 constructs provide the highest level of abstraction, also known as "patterns." Level 3 constructs mainly provide a built-in architecture pattern for our infrastructures. In our project, we used an open-source package that provides us with Level 2 and Level 3 constructs for our infrastructure. 

We created four different CloudFormation stacks to define our desired infrastructure. First, we will create a docker repository stack which creates a docker repository in AWS ECR for our application's docker images. Then, with every new release of our application, we will create a docker image with a tag for it and push it to this repository. Afterward, we will create a network stack that will create a VPC with a public and private subnet. In this stack, we will also create a loadbalancer (AWS ALB) which will be inside our public subnet, and forward all the incoming requests to the ECS cluster, which will actually run our application image inside a container environment. The private subnet will not be reachable from the outside world. Only from the public subnet will be able to communicate with any resources inside of this subnet. We will put our database inside of this subnet. That will ensure that no one will be able to access our database from outside, which is one of our project requirements.

We will create a database stack that will create a PostgreSQL database which will be used as the persistence layer of our application. As this database will be placed inside the private subnet, it will not be accessible from outside.

Finally, We will create a service stack that will create an ECS service and an ECS Task. As we only have one application to deploy, we will have only one ECS task, which will contain our application image. Our ECS service will contain a single ECS task. This ECS service will take care of the automatic scaling of our application in case of increasing loads. In case of increasing load, it will spin up EC2 compute instances for hosting the configured docker image defined on the ESC task. Behind the scene, ECS will use AWS Faragate, which is a serverless compute engine for containers. It will also take care of shutting down all those extra containers when there is not enough load. In this way, we will never have to worry about running out of computing power in case of extreme loads. Our application will always be up and running while we do not need to pay for any extra resources as our resources will be scaled based on the load.

For Continuous Integration and Continuous deployment, we used GitHub actions. Github action is event-driven means we can run series of tasks in a reaction to a specific event. An event triggers a workflow, which contains a job. A job consists of multiple steps which control the order of actions.

Workflows can contain multiple jobs which can be triggered or scheduled based on events. An event can be internal or external. A job is a set of steps. Workflow with multiple jobs by default will run those jobs on parallel, but we can change this behavior and make it sequential by making a job dependent on other jobs. This will also ensure that if a parent job fails, all the other jobs dependent on that job will not even run. The main goal of a Step is to run commands in a job. In comparison, actions are standalone commands that steps are made of. 

While designing our pipeline, we considered any push to our main branch as an event that will trigger our workflow. One of our starting jobs is to building our application and run the tests. If our tests failed or any kind of error occurred during the building of our application, we will generate a report consisting of the stacktrace and publish it as an artifact that can be downloaded anytime for debugging purposes. We also introduced caching for our application packages which will speed up the time to run our pipeline. Next, We will create a docker repository in AWS ECR for our application images. We had to provide our AWS account access key and secret access key for doing so from our pipeline. We used another feature of Github called secrets to provide those values, enabling us to use this sort of private value without exposing it. We created four of those secrets for our account id, access key id, secret access key, and AWS region. The commands that we had to run for our CDK to create and deploy our resources to AWS were becoming quite long. So we decided to create an npm repo inside the root of our CDK folder, which will help us hide those long commands behind short npm commands. Some of our CDK commands need variable parameters even that we can pass from our npm commands, making our pipeline much more readable. Also, we made this job depend on our pipeline's build job, which ensures that we will not run this job if our build job fails for any reason. The following job of our pipeline will create a network stack in our AWS account. We provide all the necessary secrets for building that stack and provide the environment as an argument in our npm command, enabling us to create different deployment environments for our application. The next step is to create a database stack. For deploying that stack, we passed two parameters, environment and application name to our CDK so that it can create a database for using that application name and using the environment variable that will be able to create that database to a specific environment. Our pipeline's last job is to build our application image and push that one to our AWS ECR repository that we created in one of the previous steps. We will build a new image for every release of our application and tag it with a unique name, which we will create using the current date and time along with the unique SHA value for that commit. Finally, we will clear the old parameter stacks that we used for publishing some of the resources ids and other properties that we wanted to share between multiple stacks.


